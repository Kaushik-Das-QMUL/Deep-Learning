{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "support_vector_machine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeYeAiG9HWxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "\n",
        "data=pd.read_csv('data.csv')\n",
        "data.drop(data.columns[0],axis=1, inplace=True) #inplace=True to physically drop the columns\n",
        "diagnosis_mapping={'M':1 ,'B':-1}               #Malignant is 1 Benign is -1 for SVM labels are +1 and -1: not 1 nd 0\n",
        "data['diagnosis']=data['diagnosis'].map(diagnosis_mapping)\n",
        "\n",
        "y=data.iloc[:,0]\n",
        "x=data.iloc[:,1:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF2LgmXeKkT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_correlated_features(X):\n",
        "\n",
        "  corr = X.corr()                                       #Variance covariance matrix is returned\n",
        "  Correlation_Threshold=.9\n",
        "  drop_columns=np.full(corr.shape[0],False, dtype=bool) #Making a list.A column drops if the value is True.\n",
        "\n",
        "  for i in range (X.shape[1]):                          #check number of Columns\n",
        "    for j in range (i+1,X.shape[1]):\n",
        "      \n",
        "      if(corr.iloc[i,j]>Correlation_Threshold):         #If the 2 columns are correalted then drop the 2nd column\n",
        "        drop_columns[j]=True\n",
        "\n",
        "  columns_to_be_dropped=X.columns[drop_columns]         #List of Column names\n",
        "  X.drop(columns_to_be_dropped, axis=1, inplace=True)   #Columns dropped Physically\n",
        "\n",
        "  return(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9_hVd__sJ32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=remove_correlated_features(x)         #Removal of Highly Correlated Features\n",
        "list_=np.zeros((X.shape[0],X.shape[1]))\n",
        "#print(list_.shape)\n",
        "j=0\n",
        "\n",
        "for col in X.columns:                   #Take attributes Data from panda Dataframe to Numpy array\n",
        "  list_[:,j]=X[col]\n",
        "  j+=1\n",
        "\n",
        "label=np.zeros(X.shape[0])\n",
        "label[:]=data['diagnosis']              #Take Y/label from panda Dataframe to Numpy array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn2iiZxzIBCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_data_with_bias(my_data):\n",
        "  mu = np.mean(my_data,axis=0)\n",
        "  std = np.std(my_data, axis=0)\n",
        "  \n",
        "  N=my_data.shape[0]\n",
        "\n",
        "  #insert extra singleton dimension, to obtain 1xD shape. Creating matrix like structure from 1-D list\n",
        "  mu_arr = np.expand_dims(mu,axis=0)\n",
        "  sig_arr = np.expand_dims(std, axis=0)\n",
        "\n",
        "  #Repeat N times to get NxD shape\n",
        "  ndarray_mu = np.repeat(mu_arr, N, axis=0)\n",
        "  ndarray_sig = np.repeat(sig_arr, N, axis=0)\n",
        "\n",
        "  normalized_data=(my_data-ndarray_mu)/ndarray_sig\n",
        "\n",
        "  bias_column=np.ones((data.shape[0],1))\n",
        "  normalized_data_with_bias=np.append(bias_column, normalized_data ,axis=1)\n",
        "\n",
        "  return (mu_arr, sig_arr, normalized_data_with_bias)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDYFKVgDO-JT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(d_train, label, weight, learning_rate,max_iteration):\n",
        "  \n",
        "  N=d_train.shape[0]\n",
        "  gutter_weight=0\n",
        "\n",
        "  for i in range (max_iteration):       #Maximum Number of Iteration\n",
        "    \n",
        "    gutter_weight = calculate_weight_using_gutter(d_train,label, weight)  \n",
        "    weight = weight-gutter_weight*learning_rate\n",
        "  print(\"Final Cost Is\",calculate_cost_after_every_iteration(d_train,label,weight))\n",
        "    \n",
        "  return weight\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGWIWHNjTsc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_weight_using_gutter(train_data,label,weight):\n",
        "\n",
        "  N=train_data.shape[0]\n",
        "  dw=0\n",
        "  for ind, val in enumerate(train_data):\n",
        "    distance = 1 - label[ind]*(np.dot(train_data[ind],weight))\n",
        "    \n",
        "    if(max(0,distance)==0):\n",
        "      di = weight\n",
        "    \n",
        "    else:\n",
        "      di = weight- lagrange_multiplier*label[ind]*train_data[ind]\n",
        "    \n",
        "    dw+=di\n",
        "  return (dw/N)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-okqJGSkedLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_cost_after_every_iteration(X,Y,W):\n",
        "  no_of_training_examples = Y.shape[0]\n",
        "\n",
        "  distances = 1 - Y * (np.dot(X, W))\n",
        "  distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
        "  hinge_loss = lagrange_multiplier * (np.sum(distances) / no_of_training_examples)\n",
        "  \n",
        "  cost = 1 / 2 * (np.dot(W, W)/(no_of_training_examples)) + hinge_loss\n",
        "  return cost\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHPvhrqfXwX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mu, sigma, d_train = normalize_data_with_bias(list_)\n",
        "print(d_train.shape)\n",
        "\n",
        "weight=np.zeros(d_train.shape[1])\n",
        "learning_rate= 0.000009\n",
        "max_iteration=2000\n",
        "lagrange_multiplier=10000\n",
        "\n",
        "print(\"splitting dataset into train and test sets...\")\n",
        "X_train, X_test, y_train, y_test = tts(d_train, label, test_size=0.2, random_state=42)\n",
        "wt=gradient_descent(X_train,y_train,weight,learning_rate,max_iteration)\n",
        "print(wt)\n",
        "\n",
        "y_train_predicted = np.array([])\n",
        "for i in range(X_train.shape[0]):\n",
        "    yp = np.sign(np.dot(X_train[i], wt))\n",
        "    y_train_predicted = np.append(y_train_predicted, yp)\n",
        "    \n",
        "print(\"*********************************************************\")\n",
        "print(\"*********************************************************\")\n",
        "print(\"*********************************************************\")\n",
        "\n",
        "print(\"accuracy on test dataset: {}\".format(accuracy_score(y_train, y_train_predicted)))\n",
        "print(\"recall on test dataset: {}\".format(recall_score(y_train, y_train_predicted)))\n",
        "print(\"precision on test dataset: {}\".format(recall_score(y_train, y_train_predicted)))\n",
        "\n",
        "print(\"*********************************************************\")\n",
        "print(\"*********************************************************\")\n",
        "print(\"*********************************************************\")\n",
        "\n",
        "y_test_predicted = np.array([])\n",
        "for i in range(X_test.shape[0]):\n",
        "    yp = np.sign(np.dot(X_test[i], wt))\n",
        "    y_test_predicted = np.append(y_test_predicted, yp)\n",
        "\n",
        "print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n",
        "print(\"recall on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))\n",
        "print(\"precision on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}