{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:50:32.422017Z","iopub.execute_input":"2021-11-20T17:50:32.422305Z","iopub.status.idle":"2021-11-20T17:50:32.427469Z","shell.execute_reply.started":"2021-11-20T17:50:32.422277Z","shell.execute_reply":"2021-11-20T17:50:32.426543Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# **SARSA**","metadata":{}},{"cell_type":"code","source":"h, w = 4, 12\nenv = np.ones((4, 12))\nenv[3, 1:11] = 250\nenv[3,0] = 120\nenv[3,11]= 150\n\ntext = np.array([['', '', '', '', '', '', '', '', '', '', '', ''], \n                 ['', '', '' ,'', '', '', '', '' ,'', '', '', ''],\n                 ['', '', '', '', '', '', '', '', '', '', '', ''],\n                 ['S', '', '', '', '', '', '', '', '', '', '', 'G']])\n\nax = sns.heatmap(env, linewidth = 2, square = True, cmap=\"rainbow_r\",\n                 linecolor = 'white', cbar = False, annot = text, fmt=\"\")\nax.axhline(y=0, color='k',linewidth= 4)\nax.axhline(y=env.shape[0], color='k',linewidth=4)\n\nax.axvline(x=0, color='k',linewidth=4)\nax.axvline(x=env.shape[1], color='k',linewidth=4)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:50:32.429152Z","iopub.execute_input":"2021-11-20T17:50:32.429620Z","iopub.status.idle":"2021-11-20T17:50:32.733368Z","shell.execute_reply.started":"2021-11-20T17:50:32.429585Z","shell.execute_reply":"2021-11-20T17:50:32.732656Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"cliff = set()\nfor i in range(1,11):\n    cliff.add((3,i))\n\nstart_point = (3, 0)\ngoal_point = (3, 11)\n\nheight = 4\nwidth = 12\n\naction_dict = {'up' : 0, 'right' : 1, 'down' : 2, 'left' : 3 }\n\ndef is_clif_or_terminal_state(x, y, is_terminal, height = 4):\n    reward = -1\n    \n    if (x, y) in cliff:\n        x, y, is_terminal, reward = height - 1, 0, False, -100\n            \n    elif (x, y) == goal_point:\n        x, y, is_terminal, reward = x, y, True, -1\n            \n    else:\n        x, y, is_terminal, reward = x, y, False, -1\n            \n    return x, y, is_terminal, reward\n\ndef determine_state(x , y, action, height = 4, width = 12):\n    is_terminal = None\n    \n    if action == 'up':\n        x = x - 1\n        if x < 0:\n            x += 1\n        x, y, is_terminal, reward = is_clif_or_terminal_state(x, y, is_terminal)\n        return x, y, is_terminal, reward\n    \n    if action == 'right':\n        y = y + 1\n        if y > (width - 1):\n            y -= 1\n        x, y, is_terminal, reward = is_clif_or_terminal_state(x, y, is_terminal)    \n        return x, y, is_terminal, reward\n    \n    if action == 'down':\n        x = x + 1\n        if x > (height - 1):\n            x -= 1\n        x, y, is_terminal, reward = is_clif_or_terminal_state(x, y, is_terminal)\n        return x, y, is_terminal, reward\n    \n    if action == 'left':\n        y = y - 1\n        if y < 0:\n            y += 1\n        x, y, is_terminal, reward = is_clif_or_terminal_state(x, y, is_terminal)\n        return x, y, is_terminal, reward\n    \n    \ndef initialise_Q_S_A():\n    Q_S_A = dict()\n    value = 0\n    for row in range(height):\n        for column in range(width):\n            for action in action_dict.keys():\n                key = (row, column, action)\n                Q_S_A[key] = 0\n    \n    return Q_S_A","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:50:32.734779Z","iopub.execute_input":"2021-11-20T17:50:32.735250Z","iopub.status.idle":"2021-11-20T17:50:32.751945Z","shell.execute_reply.started":"2021-11-20T17:50:32.735193Z","shell.execute_reply":"2021-11-20T17:50:32.750850Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import random\nall_acttion = [action for action in action_dict.keys()]\n\ndef greedy(Q,s):\n    \n    state_action_pair = {}\n    random.shuffle(all_acttion)\n    for a in all_acttion:\n        k = (s[0], s[1], a)\n        state_action_pair[k] = Q[k]\n        \n    keymax = max(state_action_pair, key = lambda x: state_action_pair[x])\n    return keymax[2]\n    \ndef epsilon_greedy(Q, s , epsilon = 0.1):\n    if np.random.uniform(0,1) < epsilon:\n        A = random.choice(all_acttion)\n    else:\n        A = greedy(Q, s)\n    return A","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:50:32.754021Z","iopub.execute_input":"2021-11-20T17:50:32.754791Z","iopub.status.idle":"2021-11-20T17:50:32.770531Z","shell.execute_reply.started":"2021-11-20T17:50:32.754746Z","shell.execute_reply":"2021-11-20T17:50:32.769527Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ntotal_trail = 100000\ngamma = 0.1\nalpha = 0.9\nQ = initialise_Q_S_A() # row, col, action\n\nfor episodes in tqdm(range(total_trail), colour='green'):\n#     current_state = (np.random.randint(0,5), np.random.randint(0,11))\n    current_state = start_point\n    action = epsilon_greedy(Q, current_state)\n    \n    is_terminal = False\n    \n    while not is_terminal:\n        \n        \"\"\"This is Next State and Reward. i.e. 'Q(S(t+1), A(t+1))', R(t+1)\"\"\"\n        x, y, is_terminal, reward = determine_state(current_state[0], \n                                                    current_state[1],\n                                                    action)\n        \n        next_action = epsilon_greedy(Q, (x,y))\n        \n        \"\"\"(x,y,next_action) is the key for the Q Dictionary\"\"\"\n        \n        Q[(current_state[0], current_state[1], action)] += gamma*(reward +\n                                                                 alpha*Q[(x,y,next_action)] -\n                                                                 Q[(current_state[0], current_state[1], action)])\n        \n        current_state = (x, y)\n        action = next_action\n","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:50:32.772134Z","iopub.execute_input":"2021-11-20T17:50:32.773308Z","iopub.status.idle":"2021-11-20T17:51:05.973017Z","shell.execute_reply.started":"2021-11-20T17:50:32.773254Z","shell.execute_reply":"2021-11-20T17:51:05.971907Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def Optimal_Policy(Q_Val):\n    \n    optimal_policy = []\n    col = ['Height', 'Width', 'Action', 'Value']\n    Q_Table = pd.DataFrame(columns = col)\n\n    for key, value in Q_Val.items():\n        Q_S_A = key[0], key[1], key[2], value\n        d = dict(zip(col, Q_S_A))\n\n        Q_Table = Q_Table.append(d, ignore_index = True)\n\n    is_terminal  = False\n    start = start_point\n\n    while not is_terminal:\n        S = start\n        H, W = S[0], S[1]\n        all_q_s_a = Q_Table[(Q_Table['Height'] == H) & (Q_Table['Width'] == W)]\n        optimal_action = all_q_s_a.loc[all_q_s_a.Value.idxmax()]\n        state_act = (optimal_action.Height, optimal_action.Width, optimal_action.Action)\n\n        optimal_policy.append(state_act)\n\n        x, y, is_terminal, _ = determine_state(optimal_action.Height ,\n                                                    optimal_action.Width,\n                                                    optimal_action.Action)\n\n        start = (x, y)\n        \n    return optimal_policy","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:51:05.974448Z","iopub.execute_input":"2021-11-20T17:51:05.974693Z","iopub.status.idle":"2021-11-20T17:51:05.985760Z","shell.execute_reply.started":"2021-11-20T17:51:05.974662Z","shell.execute_reply":"2021-11-20T17:51:05.984666Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def show_path(optimal_policy):\n\n    h, w = 4, 12\n    env = np.ones((4, 12))\n    env[3, 1:11] = 250\n    env[3,0] = 120\n    env[3,11]= 150\n\n    text = np.array([['', '', '', '', '', '', '', '', '', '', '', ''], \n                     ['', '', '' ,'', '', '', '', '' ,'', '', '', ''],\n                     ['', '', '', '', '', '', '', '', '', '', '', ''],\n                     ['S', '', '', '', '', '', '', '', '', '', '', 'G']])\n\n    direction_dictionary = {'up' : '↑', 'down' : '↓', 'right': '→', 'left': '←'}\n\n    for steps in optimal_policy:\n        x, y, directiopn = steps[0], steps[1], direction_dictionary[steps[2]]\n        text[x,y] = directiopn\n\n\n    ax = sns.heatmap(env, linewidth = 2, square = True, cmap=\"rainbow_r\",\n                     linecolor = 'white', cbar = False, annot = text, fmt = \"\",\n                    annot_kws = {\"size\": 20, \"color\" : 'black'})\n\n    ax.axhline(y=0, color='k',linewidth= 4)\n    ax.axhline(y=env.shape[0], color='k',linewidth=4)\n\n    ax.axvline(x=0, color='k',linewidth=4)\n    ax.axvline(x=env.shape[1], color='k',linewidth=4)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:51:05.987327Z","iopub.execute_input":"2021-11-20T17:51:05.987655Z","iopub.status.idle":"2021-11-20T17:51:06.003679Z","shell.execute_reply.started":"2021-11-20T17:51:05.987612Z","shell.execute_reply":"2021-11-20T17:51:06.002878Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"optimal_pol = Optimal_Policy(Q)\nshow_path(optimal_pol)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:51:06.005652Z","iopub.execute_input":"2021-11-20T17:51:06.006162Z","iopub.status.idle":"2021-11-20T17:51:06.786283Z","shell.execute_reply.started":"2021-11-20T17:51:06.006117Z","shell.execute_reply":"2021-11-20T17:51:06.785433Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# **Q Learning** : Off Policy Control Method","metadata":{}},{"cell_type":"code","source":"all_acttion = [action for action in action_dict.keys()]\n\ndef Q_max(Q_Learning, state):\n    \n    state_action_pair = {}\n    random.shuffle(all_acttion)\n    \n    for a in all_acttion:\n        k = (state[0], state[1], a)\n        state_action_pair[k] = Q[k]\n        \n    keymax = max(state_action_pair, key = lambda x: state_action_pair[x])\n    \n    \"\"\"It returns the best action at state S.\"\"\"\n    return keymax[2]\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:50:07.674471Z","iopub.execute_input":"2021-11-20T17:50:07.674785Z","iopub.status.idle":"2021-11-20T17:50:07.684084Z","shell.execute_reply.started":"2021-11-20T17:50:07.674743Z","shell.execute_reply":"2021-11-20T17:50:07.682878Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**For Q Learning, Keep gamma = 0.1, alpha = 0.9**","metadata":{}},{"cell_type":"code","source":"total_trail = 500\n\ngamma = 0.1\nalpha = 0.9\n\nQ_Learning = initialise_Q_S_A()\n\nfor episodes in tqdm(range(total_trail), colour='green'):\n\n    current_state = start_point\n\n    \n    is_terminal = False\n    \n    while not is_terminal:\n        \n        action = epsilon_greedy(Q_Learning, current_state)\n        \n        \"\"\"This is Next State and Reward. i.e. 'Q(S(t+1), A(t+1))', R(t+1)\"\"\"\n        x, y, is_terminal, reward = determine_state(current_state[0], \n                                                    current_state[1],\n                                                    action)\n        \"\"\"\n        Till this point, we have chosen Action 'a' from 'S(t)' using \n        policy derived from Q (e.g. epsilon-greedy) and we have received\n        the reward 'R'. Now we are at State : S(t+1)\n        \"\"\"\n        \n        \"\"\"\"\n        In state S(t+1) we have to select that action which has the\n        highest action value pair.\n        \"\"\"\n        next_action = Q_max(Q_Learning, (x,y))\n        \n        Q_Learning[(current_state[0], current_state[1], action)] += gamma*(reward +\n                                                                 alpha*Q_Learning[(x,y,next_action)] -\n                                                                 Q_Learning[(current_state[0], current_state[1], action)])\n        \n        current_state = (x, y)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:50:07.688680Z","iopub.execute_input":"2021-11-20T17:50:07.689166Z","iopub.status.idle":"2021-11-20T17:50:08.152882Z","shell.execute_reply.started":"2021-11-20T17:50:07.689026Z","shell.execute_reply":"2021-11-20T17:50:08.152086Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"optimal_pol = Optimal_Policy(Q_Learning)\nshow_path(optimal_pol)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:50:08.164718Z","iopub.execute_input":"2021-11-20T17:50:08.164954Z","iopub.status.idle":"2021-11-20T17:50:08.953654Z","shell.execute_reply.started":"2021-11-20T17:50:08.164927Z","shell.execute_reply":"2021-11-20T17:50:08.952990Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Expected SARSA","metadata":{}},{"cell_type":"code","source":"import random\nall_acttion = [action for action in action_dict.keys()]\n\ntotal_trail = 5000\n\ngamma = 0.1\nalpha = 0.9\nepsilon = 0.1\n\nQ_Expected = initialise_Q_S_A()\n\nfor episodes in tqdm(range(total_trail), colour='green'):\n\n    current_state = start_point\n\n    \n    is_terminal = False\n    \n    while not is_terminal:\n        \n        \"\"\"Use epsilon-greedy policy to select the current action.\"\"\"\n        action = epsilon_greedy(Q_Expected, current_state)\n        \n        \"\"\"This is Next State and Reward. i.e. 'Q(S(t+1), A(t+1))', R(t+1)\"\"\"\n        \n        x, y, is_terminal, reward = determine_state(current_state[0], \n                                                    current_state[1],\n                                                    action)\n        \"\"\"\n        Till this point, we have chosen Action 'a' from 'S(t)' using \n        policy derived from Q (e.g. epsilon-greedy) and we have received\n        the reward 'R'. Now we are at State : S(t+1)\n        \"\"\"\n        \n        \"\"\"\"\n        In state S(t+1) we have to select all actions with their probability\n        to calculate the expected return for State-Action value at time t+1;\n        i.e., p = epsilon, for the best action and \n                  (1-epsilon)/(all_action -1) for non-best actions.\n        \"\"\"\n        next_best_action = Q_max(Q_Expected, (x,y))\n        next_non_best_action = list(filter(lambda x: x != next_best_action,\n                                           all_acttion))\n        \n        probability_for_best_action = (1 - epsilon)\n        probability_for_non_best_action = epsilon/(len(all_acttion)-1)\n        \n        Q_Expected[(current_state[0], current_state[1], action)] += gamma*(reward +\n        alpha*(probability_for_best_action * Q_Expected[(x,y,next_best_action)]\n              + sum(probability_for_non_best_action * Q_Expected[(x,y,act)]\n                  for act in next_non_best_action))\n               \n              - Q_Expected[(current_state[0], current_state[1], action)])\n\n        current_state = (x, y)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:50:08.954985Z","iopub.execute_input":"2021-11-20T17:50:08.955220Z","iopub.status.idle":"2021-11-20T17:50:11.958990Z","shell.execute_reply.started":"2021-11-20T17:50:08.955191Z","shell.execute_reply":"2021-11-20T17:50:11.958090Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"optimal_pol_exp = Optimal_Policy(Q_Expected)\nshow_path(optimal_pol_exp)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:50:11.960637Z","iopub.execute_input":"2021-11-20T17:50:11.961049Z","iopub.status.idle":"2021-11-20T17:50:12.750619Z","shell.execute_reply.started":"2021-11-20T17:50:11.961004Z","shell.execute_reply":"2021-11-20T17:50:12.749621Z"},"trusted":true},"execution_count":13,"outputs":[]}]}