{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:12:23.148842Z","iopub.execute_input":"2021-11-14T19:12:23.149161Z","iopub.status.idle":"2021-11-14T19:12:24.034246Z","shell.execute_reply.started":"2021-11-14T19:12:23.149086Z","shell.execute_reply":"2021-11-14T19:12:24.032991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **SARSA**","metadata":{}},{"cell_type":"code","source":"h, w = 4, 12\nenv = np.ones((4, 12))\nenv[3, 1:11] = 250\nenv[3,0] = 120\nenv[3,11]= 150\n\ntext = np.array([['', '', '', '', '', '', '', '', '', '', '', ''], \n                 ['', '', '' ,'', '', '', '', '' ,'', '', '', ''],\n                 ['', '', '', '', '', '', '', '', '', '', '', ''],\n                 ['S', '', '', '', '', '', '', '', '', '', '', 'G']])\n\nax = sns.heatmap(env, linewidth = 2, square = True, cmap=\"rainbow_r\",\n                 linecolor = 'white', cbar = False, annot = text, fmt=\"\")\nax.axhline(y=0, color='k',linewidth= 4)\nax.axhline(y=env.shape[0], color='k',linewidth=4)\n\nax.axvline(x=0, color='k',linewidth=4)\nax.axvline(x=env.shape[1], color='k',linewidth=4)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:12:24.03615Z","iopub.execute_input":"2021-11-14T19:12:24.036423Z","iopub.status.idle":"2021-11-14T19:12:24.356849Z","shell.execute_reply.started":"2021-11-14T19:12:24.036387Z","shell.execute_reply":"2021-11-14T19:12:24.356353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cliff = set()\nfor i in range(1,11):\n    cliff.add((3,i))\n\nstart_point = (3, 0)\ngoal_point = (3, 11)\n\nheight = 4\nwidth = 12\n\naction_dict = {'up' : 0, 'right' : 1, 'down' : 2, 'left' : 3 }\n\ndef is_clif_or_terminal_state(x, y, is_terminal, height = 4):\n    reward = -1\n    \n    if (x, y) in cliff:\n        x, y, is_terminal, reward = height - 1, 0, False, -100\n            \n    elif (x, y) == goal_point:\n        x, y, is_terminal, reward = x, y, True, -1\n            \n    else:\n        x, y, is_terminal, reward = x, y, False, -1\n            \n    return x, y, is_terminal, reward\n\ndef determine_state(x , y, action, height = 4, width = 12):\n    is_terminal = None\n    \n    if action == 'up':\n        x = x - 1\n        if x < 0:\n            x += 1\n        x, y, is_terminal, reward = is_clif_or_terminal_state(x, y, is_terminal)\n        return x, y, is_terminal, reward\n    \n    if action == 'right':\n        y = y + 1\n        if y > (width - 1):\n            y -= 1\n        x, y, is_terminal, reward = is_clif_or_terminal_state(x, y, is_terminal)    \n        return x, y, is_terminal, reward\n    \n    if action == 'down':\n        x = x + 1\n        if x > (height - 1):\n            x -= 1\n        x, y, is_terminal, reward = is_clif_or_terminal_state(x, y, is_terminal)\n        return x, y, is_terminal, reward\n    \n    if action == 'left':\n        y = y - 1\n        if y < 0:\n            y += 1\n        x, y, is_terminal, reward = is_clif_or_terminal_state(x, y, is_terminal)\n        return x, y, is_terminal, reward\n    \n    \ndef initialise_Q_S_A():\n    Q_S_A = dict()\n    value = 0\n    for row in range(height):\n        for column in range(width):\n            for action in action_dict.keys():\n                key = (row, column, action)\n                Q_S_A[key] = 0\n    \n    return Q_S_A","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:12:24.357884Z","iopub.execute_input":"2021-11-14T19:12:24.358167Z","iopub.status.idle":"2021-11-14T19:12:24.371742Z","shell.execute_reply.started":"2021-11-14T19:12:24.358141Z","shell.execute_reply":"2021-11-14T19:12:24.371181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nall_acttion = [action for action in action_dict.keys()]\n\ndef greedy(Q,s):\n    \n    state_action_pair = {}\n    random.shuffle(all_acttion)\n    for a in all_acttion:\n        k = (s[0], s[1], a)\n        state_action_pair[k] = Q[k]\n        \n    keymax = max(state_action_pair, key = lambda x: state_action_pair[x])\n    return keymax[2]\n    \ndef epsilon_greedy(Q, s , epsilon = 0.1):\n    if np.random.uniform(0,1) < epsilon:\n        A = random.choice(all_acttion)\n    else:\n        A = greedy(Q, s)\n    return A","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:12:24.37323Z","iopub.execute_input":"2021-11-14T19:12:24.373676Z","iopub.status.idle":"2021-11-14T19:12:24.391879Z","shell.execute_reply.started":"2021-11-14T19:12:24.373645Z","shell.execute_reply":"2021-11-14T19:12:24.390898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ntotal_trail = 100000\ngamma = 0.1\nalpha = 0.9\nQ = initialise_Q_S_A() # row, col, action\n\nfor episodes in tqdm(range(total_trail), colour='green'):\n#     current_state = (np.random.randint(0,5), np.random.randint(0,11))\n    current_state = start_point\n    action = epsilon_greedy(Q, current_state)\n    \n    is_terminal = False\n    \n    while not is_terminal:\n        \n        \"\"\"This is Next State and Reward. i.e. 'Q(S(t+1), A(t+1))', R(t+1)\"\"\"\n        x, y, is_terminal, reward = determine_state(current_state[0], \n                                                    current_state[1],\n                                                    action)\n        \n        next_action = epsilon_greedy(Q, (x,y))\n        \n        \"\"\"(x,y,next_action) is the key for the Q Dictionary\"\"\"\n        \n        Q[(current_state[0], current_state[1], action)] += gamma*(reward +\n                                                                 alpha*Q[(x,y,next_action)] -\n                                                                 Q[(current_state[0], current_state[1], action)])\n        \n        current_state = (x, y)\n        action = next_action\n","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:12:24.393218Z","iopub.execute_input":"2021-11-14T19:12:24.393551Z","iopub.status.idle":"2021-11-14T19:12:48.376711Z","shell.execute_reply.started":"2021-11-14T19:12:24.3935Z","shell.execute_reply":"2021-11-14T19:12:48.375688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Optimal_Policy(Q_Val):\n    \n    optimal_policy = []\n    col = ['Height', 'Width', 'Action', 'Value']\n    Q_Table = pd.DataFrame(columns = col)\n\n    for key, value in Q_Val.items():\n        Q_S_A = key[0], key[1], key[2], value\n        d = dict(zip(col, Q_S_A))\n\n        Q_Table = Q_Table.append(d, ignore_index = True)\n\n    is_terminal  = False\n    start = start_point\n\n    while not is_terminal:\n        S = start\n        H, W = S[0], S[1]\n        all_q_s_a = Q_Table[(Q_Table['Height'] == H) & (Q_Table['Width'] == W)]\n        optimal_action = all_q_s_a.loc[all_q_s_a.Value.idxmax()]\n        state_act = (optimal_action.Height, optimal_action.Width, optimal_action.Action)\n\n        optimal_policy.append(state_act)\n\n        x, y, is_terminal, _ = determine_state(optimal_action.Height ,\n                                                    optimal_action.Width,\n                                                    optimal_action.Action)\n\n        start = (x, y)\n        \n    return optimal_policy","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:12:48.378044Z","iopub.execute_input":"2021-11-14T19:12:48.378329Z","iopub.status.idle":"2021-11-14T19:12:48.389101Z","shell.execute_reply.started":"2021-11-14T19:12:48.37829Z","shell.execute_reply":"2021-11-14T19:12:48.388145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_path(optimal_policy):\n\n    h, w = 4, 12\n    env = np.ones((4, 12))\n    env[3, 1:11] = 250\n    env[3,0] = 120\n    env[3,11]= 150\n\n    text = np.array([['', '', '', '', '', '', '', '', '', '', '', ''], \n                     ['', '', '' ,'', '', '', '', '' ,'', '', '', ''],\n                     ['', '', '', '', '', '', '', '', '', '', '', ''],\n                     ['S', '', '', '', '', '', '', '', '', '', '', 'G']])\n\n    direction_dictionary = {'up' : '↑', 'down' : '↓', 'right': '→', 'left': '←'}\n\n    for steps in optimal_policy:\n        x, y, directiopn = steps[0], steps[1], direction_dictionary[steps[2]]\n        text[x,y] = directiopn\n\n\n    ax = sns.heatmap(env, linewidth = 2, square = True, cmap=\"rainbow_r\",\n                     linecolor = 'white', cbar = False, annot = text, fmt = \"\",\n                    annot_kws = {\"size\": 20, \"color\" : 'black'})\n\n    ax.axhline(y=0, color='k',linewidth= 4)\n    ax.axhline(y=env.shape[0], color='k',linewidth=4)\n\n    ax.axvline(x=0, color='k',linewidth=4)\n    ax.axvline(x=env.shape[1], color='k',linewidth=4)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:12:48.390342Z","iopub.execute_input":"2021-11-14T19:12:48.390586Z","iopub.status.idle":"2021-11-14T19:12:48.410066Z","shell.execute_reply.started":"2021-11-14T19:12:48.390556Z","shell.execute_reply":"2021-11-14T19:12:48.408928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimal_pol = Optimal_Policy(Q)\nshow_path(optimal_pol)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:12:48.41185Z","iopub.execute_input":"2021-11-14T19:12:48.412105Z","iopub.status.idle":"2021-11-14T19:12:49.087192Z","shell.execute_reply.started":"2021-11-14T19:12:48.412077Z","shell.execute_reply":"2021-11-14T19:12:49.086738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Q Learning** : Off Policy Control Method","metadata":{}},{"cell_type":"code","source":"all_acttion = [action for action in action_dict.keys()]\n\ndef Q_max(Q_Learning, state):\n    \n    state_action_pair = {}\n    random.shuffle(all_acttion)\n    \n    for a in all_acttion:\n        k = (state[0], state[1], a)\n        state_action_pair[k] = Q[k]\n        \n    keymax = max(state_action_pair, key = lambda x: state_action_pair[x])\n    \n    \"\"\"It returns the best action at state S.\"\"\"\n    return keymax[2]\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:13:22.421605Z","iopub.execute_input":"2021-11-14T19:13:22.422245Z","iopub.status.idle":"2021-11-14T19:13:22.427337Z","shell.execute_reply.started":"2021-11-14T19:13:22.422218Z","shell.execute_reply":"2021-11-14T19:13:22.426655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For Q Learning, Keep gamma = 0.1, alpha = 0.9**","metadata":{}},{"cell_type":"code","source":"total_trail = 500\n\ngamma = 0.1\nalpha = 0.9\n\nQ_Learning = initialise_Q_S_A()\n\nfor episodes in tqdm(range(total_trail), colour='green'):\n\n    current_state = start_point\n\n    \n    is_terminal = False\n    \n    while not is_terminal:\n        \n        action = epsilon_greedy(Q_Learning, current_state)\n        \n        \"\"\"This is Next State and Reward. i.e. 'Q(S(t+1), A(t+1))', R(t+1)\"\"\"\n        x, y, is_terminal, reward = determine_state(current_state[0], \n                                                    current_state[1],\n                                                    action)\n        \"\"\"\n        Till this point, we have chosen Action 'a' from 'S(t)' using \n        policy derived from Q (e.g. epsilon-greedy) and we have received\n        the reward 'R'. Now we are at State : S(t+1)\n        \"\"\"\n        \n        \"\"\"\"\n        In state S(t+1) we have to select that action which has the\n        highest action value pair.\n        \"\"\"\n        next_action = Q_max(Q_Learning, (x,y))\n        \n        Q_Learning[(current_state[0], current_state[1], action)] += gamma*(reward +\n                                                                 alpha*Q_Learning[(x,y,next_action)] -\n                                                                 Q_Learning[(current_state[0], current_state[1], action)])\n        \n        current_state = (x, y)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:13:25.080428Z","iopub.execute_input":"2021-11-14T19:13:25.080911Z","iopub.status.idle":"2021-11-14T19:13:25.373087Z","shell.execute_reply.started":"2021-11-14T19:13:25.080875Z","shell.execute_reply":"2021-11-14T19:13:25.372225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimal_pol = Optimal_Policy(Q_Learning)\nshow_path(optimal_pol)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:13:28.72713Z","iopub.execute_input":"2021-11-14T19:13:28.727532Z","iopub.status.idle":"2021-11-14T19:13:29.266368Z","shell.execute_reply.started":"2021-11-14T19:13:28.727487Z","shell.execute_reply":"2021-11-14T19:13:29.265863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Expected SARSA","metadata":{}},{"cell_type":"code","source":"import random\nall_acttion = [action for action in action_dict.keys()]\n\ntotal_trail = 5000\n\ngamma = 0.1\nalpha = 0.9\nepsilon = 0.1\n\nQ_Expected = initialise_Q_S_A()\n\nfor episodes in tqdm(range(total_trail), colour='green'):\n\n    current_state = start_point\n\n    \n    is_terminal = False\n    \n    while not is_terminal:\n        \n        \"\"\"Use epsilon-greedy policy to select the current action.\"\"\"\n        action = epsilon_greedy(Q_Expected, current_state)\n        \n        \"\"\"This is Next State and Reward. i.e. 'Q(S(t+1), A(t+1))', R(t+1)\"\"\"\n        \n        x, y, is_terminal, reward = determine_state(current_state[0], \n                                                    current_state[1],\n                                                    action)\n        \"\"\"\n        Till this point, we have chosen Action 'a' from 'S(t)' using \n        policy derived from Q (e.g. epsilon-greedy) and we have received\n        the reward 'R'. Now we are at State : S(t+1)\n        \"\"\"\n        \n        \"\"\"\"\n        In state S(t+1) we have to select all actions with their probability\n        to calculate the expected return for State-Action value at time t+1;\n        i.e., p = epsilon, for the best action and \n                  (1-epsilon)/(all_action -1) for non-best actions.\n        \"\"\"\n        next_best_action = Q_max(Q_Expected, (x,y))\n        next_non_best_action = list(filter(lambda x: x != next_best_action,\n                                           all_acttion))\n        \n        probability_for_best_action = (1 - epsilon)\n        probability_for_non_best_action = epsilon/(len(all_acttion)-1)\n        \n        Q_Expected[(current_state[0], current_state[1], action)] += gamma*(reward +\n        alpha*(probability_for_best_action * Q_Expected[(x,y,next_best_action)]\n              + sum(probability_for_non_best_action * Q_Expected[(x,y,act)]\n                  for act in next_non_best_action))\n               \n              - Q_Expected[(current_state[0], current_state[1], action)])\n\n        current_state = (x, y)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:13:34.202375Z","iopub.execute_input":"2021-11-14T19:13:34.203285Z","iopub.status.idle":"2021-11-14T19:13:36.674388Z","shell.execute_reply.started":"2021-11-14T19:13:34.203253Z","shell.execute_reply":"2021-11-14T19:13:36.673531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimal_pol_exp = Optimal_Policy(Q_Expected)\nshow_path(optimal_pol_exp)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:13:39.016832Z","iopub.execute_input":"2021-11-14T19:13:39.017087Z","iopub.status.idle":"2021-11-14T19:13:39.725116Z","shell.execute_reply.started":"2021-11-14T19:13:39.01705Z","shell.execute_reply":"2021-11-14T19:13:39.72431Z"},"trusted":true},"execution_count":null,"outputs":[]}]}