{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_regression",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkuaydNouvcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "my_data=pd.read_csv('random_data.csv')\n",
        "my_data_refined=my_data.drop(['Id'], axis=1)\n",
        "print(my_data_refined.head())\n",
        "\n",
        "list_=np.zeros((my_data_refined.shape[0],my_data_refined.shape[1]-1)) #Shape[0] gives number of rows\n",
        "list_[:, 0]=my_data_refined[\"x1\"]\n",
        "list_[:, 1]=my_data_refined[\"x2\"]\n",
        "list_[:, 2]=my_data_refined[\"x3\"]\n",
        "list_[:, 3]=my_data_refined[\"x4\"]\n",
        "target=np.zeros((my_data_refined.shape[0],1))\n",
        "\n",
        "#print(len(list_))\n",
        "target[:,0]=my_data_refined[\"y\"]\n",
        "print(len(target))\n",
        "def Data_Normalization(X):\n",
        "  mean=np.mean(X, axis=0)\n",
        "  sd= np.std(X, axis=0)\n",
        "  \n",
        "  N=X.shape[0] #Number of Examples\n",
        "\n",
        "  # insert extra singleton dimension, to obtain 1xD shape i.e to bypass the shape (4,): we want (1,4)\n",
        "\n",
        "  mean_vector = np.expand_dims(mean, axis=0)\n",
        "  std_vector = np.expand_dims(sd, axis=0)\n",
        "  \n",
        "  #Repeat N times to get NxD shape\n",
        "  Ndim_mean = np.repeat(mean_vector, N, axis=0)\n",
        "  Ndim_std = np.repeat(std_vector, N, axis=0)\n",
        "\n",
        "  if(np.any(Ndim_std)==0):\n",
        "    Ndim_std+=.000000001\n",
        "\n",
        "  X_Normalized=(X-Ndim_mean)/Ndim_std\n",
        "\n",
        "  return(X_Normalized, mean_vector, std_vector)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuWl1_2_1agy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "data, mu, sigma = Data_Normalization(list_) #Getting the normalized data\n",
        "bias_column=np.ones((data.shape[0],1))      #creating the bias term\n",
        "data=np.append(bias_column,data, axis=1)    #adding the bias term\n",
        "theta=np.zeros(data.shape[1])               #Creating a rank 1 array\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPKOonsV4Bnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_hypothesis(data,theta,i):\n",
        "  h_theta=np.dot(theta,data[i])\n",
        "  return(h_theta)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLxUs2CA-kP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_theta(data,y,learning_rate,theta,max_iteration):\n",
        "\n",
        "  cost_vector=np.array([], dtype=np.float32)\n",
        "  no_of_examples=data.shape[0]                      #Number of Examples extracted\n",
        "  for i in range (max_iteration):                   #Number of Epoch\n",
        "\n",
        "    theta_temp=theta.copy()\n",
        "    sum_over_del_j_theta_d_theta_k=np.zeros(no_of_examples)\n",
        "    for j in range (no_of_examples):                #For all examples\n",
        "\n",
        "      hypothesis=calculate_hypothesis(data,theta,j) #for a particular example calculating prediction\n",
        "\n",
        "      for k in range (len(theta)):                  #For all theta and all examples we are calculating the sum_over_del_j_theta_d_theta_k\n",
        "        sum_over_del_j_theta_d_theta_k[k] +=(hypothesis-y[j])*data[j,k]    #jth row of kth example :data[j,k] \n",
        "\n",
        "    for k in range (len(theta)):                    #Calculating Theta : to Update Simultaneously\n",
        "      theta_temp[k]= theta_temp[k] - ((learning_rate)/no_of_examples)*sum_over_del_j_theta_d_theta_k[k]\n",
        "    \n",
        "    theta=theta_temp.copy()                         #Update Simultaneously\n",
        "\n",
        "    cost_after_this_iteration = calculate_cost(data,y,theta) #Calculating cost for visual input\n",
        "    print(\"After iteration {} cost is {:f}\".format(i,float(cost_after_this_iteration)))\n",
        "    cost_vector = np.append(cost_vector, cost_after_this_iteration)\n",
        "\n",
        "    if (cost_after_this_iteration<.0001):\n",
        "      break\n",
        "  plot_cost(cost_vector)\n",
        "  return (theta)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2dAtf_iF_EU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_cost(data,y,theta):\n",
        "  no_of_training_examples=y.shape[0]\n",
        "  avg_cost=0\n",
        "  sum=0\n",
        "  for i in range (no_of_training_examples):\n",
        "    prediction=calculate_hypothesis(data,theta,i)\n",
        "    \n",
        "    target=y[i]\n",
        "    j_theta= (target-prediction)**2\n",
        "    \n",
        "    sum+=j_theta\n",
        "    #print(i)\n",
        "  avg_cost=sum/(2*no_of_training_examples)\n",
        "  \n",
        "  return avg_cost\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ile4XLy7TQMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_cost(cost):\n",
        "  figure,axis=plt.subplots()\n",
        "  axis.set_xlabel('Itearion')\n",
        "  axis.set_ylabel('Cost')\n",
        "  plt.plot(cost)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sz_InSRORiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exact_solution(data,target):\n",
        "  theta=np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(data),data)),np.transpose(data)),target)\n",
        "  Theta_=theta.ravel()  #ND array to 1D list\n",
        "  return Theta_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2Ic9BBsHMHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Theta=calculate_theta(data,target,.1,theta,1000)\n",
        "Theta_=exact_solution(data,target)\n",
        "print(Theta)\n",
        "print(Theta_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiyh9PeOZ126",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c9ec9229-aaa6-4000-dbcf-e3e8424fdd8d"
      },
      "source": [
        "data_=np.array([[85, 2, 15, -0.170014769]])\n",
        "data_=(data_-mu)/sigma\n",
        "column_of_ones = np.ones((data_.shape[0], 1))\n",
        "data_with_bias = np.append(column_of_ones, data_, axis=1)\n",
        "print(\"Prediction using Gradient Descent is \",calculate_hypothesis(data_with_bias,Theta,0))\n",
        "print(\"Prediction using linear Algebra is \",calculate_hypothesis(data_with_bias,Theta_,0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction using Gradient Descent is  296.35714605450977\n",
            "Prediction using linear Algebra is  296.3680298844417\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}